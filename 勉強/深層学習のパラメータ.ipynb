{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習のパラメータについて\n",
    "\n",
    "Kerasにおけるパラメータとしてどんなものがあって、どのような特性を持つのかをまとめておきます。 \n",
    "\n",
    "## 参考\n",
    "\n",
    "* [無から始めるKeras](https://qiita.com/Ishotihadus/items/d47fc294ca568536b7f0)\n",
    "* [ニューラルネットワークのバリエーション](https://thinkit.co.jp/story/2015/09/09/6399)\n",
    "\n",
    "## サンプルコード\n",
    "\n",
    "サンプルコードは↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X = [0,0,0]\n",
    "y = [1]\n",
    "model = tf.keras.models.Sequential()\t# Sequentialモデル\n",
    "# 入力2/出力8/活性化関数tanhの層を追加\n",
    "model.add(tf.keras.layers.Dense(1024,input_dim=3, activation='tanh'))\n",
    "# 入力8(省略)/出力1/活性化関数sigmoidの層を追加\n",
    "model.add(tf.keras.layers.Dense(1, activation='softplus'))\n",
    "# 学習の仕方を定義\n",
    "# * 目的関数(ロス関数)としてbinary_crossentropy(logloss)\n",
    "# * 最適化アルゴリズムにSGD(確率的勾配降下法）、学習率を0.1に設定\n",
    "# * 評価メトリクスとしてaccuracyを表示\n",
    "model.compile(\"adamax\", loss='kld',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 活性化関数:Activation\n",
    "\n",
    "![](https://thinkit.co.jp/sites/default/files/639904.png)\n",
    "\n",
    "## -1から1の間をとるもの\n",
    "\n",
    "* softsign\n",
    "* tanh\n",
    "\n",
    "tanhのほうが変化が急峻。\n",
    "\n",
    "## 0以上1以下のみをとるもの\n",
    "\n",
    "* sigmoid\n",
    "* hard_sigmoid\n",
    "\n",
    "ハードシグモイドのほうが急で折れ線。シグモイドは滑らか。\n",
    "入力が0のとき出力は0.5。上と同様0あたりでは線形に効くが、絶対値が大きくなってくると0か1に張り付く。\n",
    "\n",
    "## 0以上の値をとるもの\n",
    "\n",
    "* softplus\n",
    "* relu\n",
    "\n",
    "reluほうが急で折れ線。ソフトプラスは（かなり）滑らか。\n",
    "値がある程度（ゼロから見て）大きくなると線形になり、小さくなるとゼロに近くなる感じ。\n",
    "reluは係数で0未満の値をとることもできる。\n",
    "\n",
    "## その他\n",
    "\n",
    "* 線形関数（linear）\n",
    "* ソフトマックス（softmax）\n",
    "\n",
    "線形の方は単純に係数をかけてバイアスを加える（係数とバイアスは学習される？）。ただ活性化関数が線形なのはあまり意味がない気がするのでこれはあまり使わないのかも。\n",
    "\n",
    "ソフトマックスは、その層から出力されたすべての値に指数関数をかけ（すなわちすべて正にし）、その和を1に正規化する。この関数がかかった後、和は1だしどの値も正なので、確率として判断することができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的関数:Objective\n",
    "\n",
    "compile の2つ目の引数。小さくしたい関数。誤差（loss）とだいたい同じ意味。  \n",
    "「どんな基準で誤差を決めるか」を決める。  \n",
    "ここでいう「誤差」とは、今のニューラルネットから推測される値と正解の値の差のことをいう。\n",
    "\n",
    "種類としては下記がある。\n",
    "* 平均二乗誤差（mse：差の2乗の和）\n",
    "* 平均絶対誤差（msa：差の絶対値の和）\n",
    "* 平均絶対誤差率（mspa：差を正解の値で割った値（誤差率）の絶対値の和）\n",
    "* 対数平均二乗誤差（msle：「1を加えた値の対数」の差の2乗の和）\n",
    "* ヒンジ損失の和（hinge）\n",
    "* ヒンジ損失の二乗の和（squared_hinge）\n",
    "* 2クラス分類時の交差エントロピー（binary_crossentropy）\n",
    "* Nクラス分類時の交差エントロピー（categorical_crossentropy）\n",
    "* スパースなNクラス分類交差エントロピー（sparse_categorical_crossentropy）\n",
    "* KLダイバージェンス（kld）\n",
    "* poisson\n",
    "* コサイン類似度を負にしたもの（cosine_proximity）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最適化関数:Optimizer\n",
    "\n",
    "最適化関数とは、誤差関数の極小値を探索するアルゴリズムです。  \n",
    "誤差をニューラルネットワークの出力と正解の差とすると、誤差が少なくなるように探索するのに、誤差関数の極小値を探索すればよいことになります。  \n",
    "基本的な考え方は、例えば3次元の誤差関数とすれば、ボールを置いてどこまで転がるか、で評価できます。(勾配をどれだけ降下するか→勾配降下法)\n",
    "\n",
    "![Gif](http://1.bp.blogspot.com/-K_X-yud8nj8/VPmIBxwGlsI/AAAAAAAACC0/JS-h1fa09EQ/s1600/Saddle%2BPoint%2B-%2BImgur.gif)\n",
    "\n",
    "## sgd:確率的勾配降下法\n",
    "\n",
    "Stochastic Gradient Descentの略で、Optimizerの中でも初期に提唱された最も基本的なアルゴリズムです。  \n",
    "最も基本になるので、ちょっと解説を書いておきます。\n",
    "\n",
    "重み$w$の更新は以下のように行っています。このとき、$E$は誤差関数、$\\eta$は学習係数を表しています。\n",
    "\n",
    "${\\mathbf{w}^{t + 1} \\gets \\mathbf{w}^{t} - \\eta \\frac{\\partial E(\\mathbf{w}^{t})}{\\partial \\mathbf{w}^{t}}\n",
    "}$\n",
    "\n",
    "この手法は、通常の勾配法で起こりうる局所最適解への収束という問題点を、$w$の更新ごとにランダムにサンプルを選び出すことで解消したものです。  \n",
    "また、訓練データにおける冗長性を効率良く学習することができるという利点もあります。  \n",
    "しかし学習係数$\\eta$を恣意的に決定する必要があり、一度決めた$\\eta$を用いて誤差の最小化を行なっていくことになります。  \n",
    "このことから、学習モデルによって最適なパラメータを決めることが難しいという問題があります。\n",
    "\n",
    "## 主だったもの\n",
    "\n",
    "表に列挙していきます。\n",
    "\n",
    "|kerasのパラメータ名|アルゴリズム名|日本語訳|パラメータ|特徴|\n",
    "|-----------------|-----------|---------|-----------|---|\n",
    "|sgd              |Stochastic Gradient Descent|確率的勾配降下法|$\\eta$:学習係数,$\\alpha$:慣性項(momentum)||\n",
    "|adagrad          |AdaGrad                    |?            |初期学習率$\\eta$,発散抑制:$\\epsilon$|SGDから学習係数を動的に変化するようにしたもの|\n",
    "|rsmprop          |RSMprop                    |?            |初期学習率$\\eta$発散抑制:$\\epsilon$,$\\alpha$|AdaGradに勾配の2乗の指数移動平均を使って最適化する形に改良|\n",
    "|adadelta         |AdaDelta                   |?            |$\\rho$,発散抑制:$\\epsilon$|直近の勾配情報を優先するようAdaGradを改善し初期学習率をパラメータから削除|\n",
    "|adam             |Adaptive moment estimation |?            |$\\alpha$, $\\beta_{1}$, $\\beta_{2}$, $\\epsilon$|移動指数平均により収束率を改善|\n",
    "|adamax           |?                          |?            |?                                |?|\n",
    "|nadam            |Nesterov Adam              |?            |?                                |?|\n",
    "\n",
    "といったものがあります。\n",
    "\n",
    "## よく使うもの\n",
    "\n",
    "よくわからないが、よくわからないなりに使うには、$\\eta$がなくともある程度収束が速いadamを使うと良さげらしい。  \n",
    "RNNを使う場合、adamだと収束がだいぶ遅くなるのでrsmpropを使うのが定説らしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ネットワークの種類\n",
    "\n",
    "大きく分けて教師あり学習/教師なし学習に分類されるが、今回は教師あり学習で使われるものについてまとめておく。\n",
    "\n",
    "## 全結合\n",
    "\n",
    "Dense層を使ったもの。  \n",
    "音声解析等に利用されているらしい。\n",
    "\n",
    "<img src=\"https://thinkit.co.jp/sites/default/files/639908.png\" width=50%>\n",
    "\n",
    "## CNN\n",
    "\n",
    "Convolutional Neural Networkの略。畳み込みニューラルネットワークとも。  \n",
    "画像解析に利用されている。  \n",
    "Kerasでは畳み込み層がconv1d/conv2d、プーリング層はMaxPooling1D等を使う。\n",
    "\n",
    "<img src=\"https://thinkit.co.jp/sites/default/files/639906.png\" width=50%>\n",
    "\n",
    "## RNN\n",
    "\n",
    "Recurrent Neural Networkの略。再帰的ニューラルネットとも。  \n",
    "データの順番を意識して学習してくれるため、時系列データや自然言語処理に利用されている。  \n",
    "どの程度前の入力を保持するかによってアルゴリズムが多数あるが、LSTM(Long Short-Term Memory)がよく使われているらしい。\n",
    "\n",
    "<img src=\"https://thinkit.co.jp/sites/default/files/639907.png\" width=50%>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
